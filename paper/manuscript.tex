% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{social cognition, individual differences, gaze cues, cognitive modeling\newline\indent Word count: xxx}
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage{setspace}
\captionsetup[figure]{font={stretch=1}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Variation in gaze understanding across the life span: A process-level perspective},
  pdfauthor={Julia Prein1, Manuel Bohn1,2, Luke Maurits1, Annika Werwach1, \& Daniel B. M. Haun1},
  pdflang={en-EN},
  pdfkeywords={social cognition, individual differences, gaze cues, cognitive modeling},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Variation in gaze understanding across the life span: A process-level perspective}
\author{Julia Prein\textsuperscript{1}, Manuel Bohn\textsuperscript{1,2}, Luke Maurits\textsuperscript{1}, Annika Werwach\textsuperscript{1}, \& Daniel B. M. Haun\textsuperscript{1}}
\date{}


\shorttitle{modeling variation in gaze understanding}

\authornote{

The authors made the following contributions. Julia Prein: Conceptualization, Methodology, Software, Investigation, Formal Analysis, Writing - Original Draft Preparation, Writing - Review \& Editing; Manuel Bohn: Conceptualization, Formal Analysis, Writing - Original Draft Preparation, Writing - Review \& Editing; Luke Maurits: Formal Analysis, Writing - Review \& Editing; Annika Werwach: Methodology, Investigation, Writing - Review \& Editing; Daniel B. M. Haun: Supervision, Writing - Review \& Editing.

Correspondence concerning this article should be addressed to Julia Prein, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, 04103 Leipzig, Germany. E-mail: \href{mailto:julia_prein@eva.mpg.de}{\nolinkurl{julia\_prein@eva.mpg.de}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Department of Comparative Cultural Psychology, Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany\\\textsuperscript{2} Institute of Psychology, Leuphana University LÃ¼neburg, Germany}

\abstract{%
Abstract
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{itemize}
\tightlist
\item
  why do we care about developmental trajectory? ref to stat learning paper
\item
  variation
\end{itemize}

\hypertarget{why-do-we-need-gaze-understanding}{%
\subsection{Why do we need gaze understanding?}\label{why-do-we-need-gaze-understanding}}

How do humans learn about their environment and navigate through their social surroundings?
One possibility to extract information from the environment is through following others' focus of attention.
Building a common ground is considered especially important in communicative interactions and shared activities (Tomasello, Hare, Lehmann, \& Call, 2007).

\hypertarget{how-does-gaze-following-emerge}{%
\subsection{How does gaze following emerge?}\label{how-does-gaze-following-emerge}}

Existing studies operationalize gaze following as the ability to follow another agent's line of sight.
As one of the most fundamental social-cognitive abilities, it has been extensively studied in infancy and early childhood.
Infants as young as six months can attune their gaze to that of another agent (D'Entremont, Hains, \& Muir, 1997).
At the end of their first year of life, infants can follow gaze to locations outside their current visual field and move themselves to gain proper perceptual access (Moll \& Tomasello, 2004).

While the emergence of gaze following has been well established, less is known about the developmental trajectory throughout childhood and adolescence.
One possibility is that our social-cognitive ability in question is fully developed once emerged in infancy.However, many cognitive abilities develop with age (e.g., working memory, Gathercole, Pickering, Ambridge, \& Wearing, 2004).
Similarly, visual processing appears to improve with age.
Therefore, children could potentially improve in gaze following, fine-tuning the performance of the already existing skill.

\hypertarget{the-scope-of-infants-gaze-following-ability}{%
\subsection{The scope of infants' gaze following ability}\label{the-scope-of-infants-gaze-following-ability}}

Though these studies suggest that young infants can align their visual attention to another's line of sight, it does not necessarily include understanding the intentions of the other agent.
Infants could simply attune their orientation or be attracted by others' gaze without processing what exactly the other is seeing (cf.~Butterworth \& Jarrett's ecological and geometric mechanism, Butterworth and Jarrett (1991){]}. Therefore, it is crucial to study children's intentional understanding of gaze.

Moore, Angelopoulos, and Bennett (1997) showed that 9-month-olds followed an agent's gaze more, when it was accompanied by a dynamic head turn in comparison to a static head turn.

In a hiding game with two search locations, Povinelli, Reaux, Bierschwale, Allain, and Simon (1997) found that three-year-olds used gaze as a cue to locate the reward, while two-year-olds performed at chance level.

In a similar object choice paradigm with two containers, Behne, Carpenter, and Tomasello (2005) investigated whether infants understand the communicative intent behind pointing and gaze cues.
In contrast to Povinelli et al. (1997), they found that already 14-month-olds used the agent's cues to select an object.
In conditions with absent-minded `cues', infants performed around chance.
This could be interpreted as infants recognizing the nature of this joint activity: namely, that the adult's behavior was beneficial and relevant for their object choice.

\hypertarget{head-vs-eye-direction}{%
\subsubsection{Head vs eye direction}\label{head-vs-eye-direction}}

It is important to note that in many existing gaze conditions, the experimenter shifted their eyes and head in synchrony (e.g., Behne et al. (2005)).
Instead of pointing towards gaze understanding, a critic could claim that the results can be explained by face direction alone.

A handful of studies approached this potential confound by separately manipulating head and eye movement.
Brooks and Meltzoff (2002) implemented a comparison between eye and head orientation and found that 14-month-olds were sensitive to open versus closed eyes.

Investigating the `cooperative eye hypothesis', Tomasello et al. (2007) implemented six conditions, in which an experimenter oriented towards the ceiling with their eyes only, head only (eyes closed), both head and eyes, or neither.
They found that human infants relied more on the eye movement, while chimpanzees paid more attention to the head movement.

Importantly, the subjects were not presented with an object choice but their attention orientation was measured.

\begin{itemize}
\item
  (Raviv \& Arnon, 2018)
\item
  (Astor \& GredebÃ¤ck, 2022)
\item
  (Colombo, 2001)
\item
  (Scaife \& Bruner, 1975)
\item
  (Itakura \& Tanaka, 1998)
\item
  (Carpenter, Nagell, \& Tomasello, 1998) ``Several other studies have attempted to determine more precisely the cue that infants are using when they follow the gaze direction of others, that is, whether they use adults' head or eye orientation. In tasks comparing infants' responses when the experimenters turned their head and eyes together to targets with their responses when the experimenters directed their eyes to the targets but their head remained facing forward, Corkum and Moore (1995), Lempers (1979), and Lempers, Flavell, and Flavell (1977) all found that only infants age 12 months and older responded correctly when eyes and head were oriented in the same direction and that infants at all ages (i.e., through 19 months) performed poorly when eye and head direction diverged'' (p.10-11) object choice.
\item
  (Silverstein, Feng, Westermann, Parise, \& Twomey, 2021) for vertical plane
\item
  (Zhang, Zhang, Zhang, Tang, \& Liu, 2019)
\item
  (Frischen, Bayliss, \& Tipper, 2007)
\item
  (Lee, Eskritt, Symons, \& Muir, 1998)
\item
  (Coelho, George, Conty, Hugueville, \& Tijus, 2006)
\end{itemize}

\hypertarget{aim-of-the-current-project}{%
\subsection{Aim of the current project}\label{aim-of-the-current-project}}

\hypertarget{developmental-trajectory-measuring-modeling-individual-differences}{%
\subsubsection{Developmental trajectory, measuring \& modeling individual differences}\label{developmental-trajectory-measuring-modeling-individual-differences}}

In this study, we were interested in the developmental trajectory of gaze understanding.
While we expect the younger children to be able to follow gaze, we aimed at assessing the differentiation of their social-cognitive ability.
Our goal was \emph{not} to establish the youngest age at which children understand gaze cues.
Rather, we wanted to examine how that ability changes with age.

In our study, we focused on the communicative intents of gaze: we asked children to locate a target by following an agent's gaze.
While language demands were kept low, the participants had to actively respond and, therefore, make use of the presented gaze cue.

A unique contribution of this study is the richness of the data set.
Methodological challenges arise when trying to compare data across ages from qualitatively and quantitatively different study tasks.
We could circumvent these issues by applying the exact same task for the entire life span.

\hypertarget{lifespan}{%
\section{Lifespan}\label{lifespan}}

\begin{itemize}
\tightlist
\item
  development \& individual differences in gaze understanding
\item
  verweis methods paper reliable differences kinder \& adults.
\item
  kontinuerliche, systematische variation, wodrin? =\textgreater{} model
\end{itemize}

\url{https://osf.io/6yjz3}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:lifespan_sample}}

\begin{tabular}{lllll}
\toprule
Age group & \multicolumn{1}{c}{n} & \multicolumn{1}{c}{Age mean} & \multicolumn{1}{c}{Age range} & \multicolumn{1}{c}{Age SD}\\
\midrule
3.00 & 19 (7 female) & 3.62 & 3.04 - 3.99 & 0.31\\
4.00 & 17 (9 female) & 4.45 & 4.05 - 4.91 & 0.30\\
5.00 & 22 (13 female) & 5.56 & 5.08 - 5.99 & 0.31\\
6.00 & 24 (16 female) & 6.50 & 6.1 - 6.99 & 0.28\\
7.00 & 39 (20 female) & 7.48 & 7.04 - 7.95 & 0.25\\
8.00 & 41 (20 female) & 8.46 & 8.03 - 8.98 & 0.27\\
9.00 & 56 (29 female) & 9.46 & 9.01 - 9.96 & 0.28\\
10.00 & 35 (22 female) & 10.49 & 10.01 - 11 & 0.28\\
11.00 & 54 (26 female) & 11.43 & 11.01 - 11.96 & 0.28\\
12.00 & 43 (19 female) & 12.41 & 12.01 - 12.99 & 0.30\\
13.00 & 42 (19 female) & 13.50 & 13.09 - 13.99 & 0.27\\
14.00 & 20 (14 female) & 14.37 & 14.05 - 14.98 & 0.23\\
15.00 & 21 (11 female) & 15.56 & 15.05 - 15.98 & 0.30\\
16.00 & 19 (10 female) & 16.51 & 16.17 - 16.97 & 0.24\\
17.00 & 19 (10 female) & 17.53 & 17.01 - 17.95 & 0.28\\
18.00 & 2 (0 female) & 18.00 & 18 - 18 & 0.00\\
19.00 & 5 (4 female) & 19.00 & 19 - 19 & 0.00\\
20.00 & 40 (25 female) & 23.02 & 20 - 29 & 2.77\\
30.00 & 40 (21 female) & 34.42 & 30 - 39 & 3.00\\
40.00 & 40 (24 female) & 44.17 & 40 - 49 & 2.92\\
50.00 & 40 (21 female) & 54.38 & 50 - 59 & 3.04\\
60.00 & 40 (21 female) & 63.73 & 60 - 69 & 2.56\\
70.00 & 40 (20 female) & 72.75 & 70 - 79 & 2.44\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

We collected data from a remote child, teenager and adult sample.
For the remote child and teenager sample, we recruited participants via an internal database consisting of families living in Leipzig, Germany, who volunteered to participate in child development studies and indicated an interest in online studies.

The remote child and teenager sample consisted of 471 children, including 19 3-year-olds (mean = 3.62 years, SD = 0.31, range = 3.04 - 3.99, 7 girls), 17 4-year-olds (mean = 4.45 years, SD = 0.30, range = 4.05 - 4.91, 9 girls), 22 5-year-olds (mean = 5.56 years, SD = 0.31, range = 5.08 - 5.99, 13 girls), 24 6-year-olds (mean = 6.50 years, SD = 0.28, range = 6.10 - 6.99, 16 girls), 39 7-year-olds (mean = 7.48 years, SD = 0.25, range = 7.04 - 7.95, 20 girls), 41 8-year-olds (mean = 8.46 years, SD = 0.27, range = 8.03 - 8.98, 20 girls), 56 9-year-olds (mean = 9.46 years, SD = 0.28, range = 9.01 - 9.96, 29 girls), 35 10-year-olds (mean = 10.49 years, SD = 0.28, range = 10.01 - 11, 22 girls), 54 11-year-olds (mean = 11.43 years, SD = 0.28, range = 11.01 - 11.96, 26 girls), 43 12-year-olds (mean = 12.41 years, SD = 0.30, range = 12.01 - 12.99, 19 girls), 42 13-year-olds (mean = 13.50 years, SD = 0.27, range = 13.09 - 13.99, 19 girls), 20 14-year-olds (mean = 14.37 years, SD = 0.23, range = 14.05 - 14.98, 14 girls), 21 15-year-olds (mean = 15.56 years, SD = 0.30, range = 15.05 - 15.98, 11 girls), 19 16-year-olds (mean = 16.51 years, SD = 0.24, range = 16.17 - 16.97, 10 girls), 19 17-year-olds (mean = 17.53 years, SD = 0.28, range = 17.01 - 17.95, 10 girls), 2 17-year-olds (mean = 18 years, SD = 0, range = 18 - 18, 0 girls),
5 17-year-olds (mean = 19 years, SD = 0, range = 19 - 19, 4 girls).

Children and teenagers in our sample grow up in an industrialized, urban Central-European context.
Information on socioeconomic status was not formally recorded, although the majority of families come from mixed, mainly mid to high socioeconomic backgrounds with high levels of parental education.

Adults were recruited via \emph{Prolific} (Palan \& Schitter, 2018). \emph{Prolific} is an online participant recruitment service from the University of Oxford with a predominantly European and US-American subject pool. Participants consisted of 240 English-speaking adults that reported to have normal or corrected-to-normal vision. The sample included 40 20- to 29-year-olds (mean = 23.02 years, SD = 2.77, range = 20 - 29, 25 female),
40 30- to 39-year-olds (mean = 34.42 years, SD = 3, range = 30 - 39, 21 female),
40 40- to 49-year-olds (mean = 44.17 years, SD = 2.92, range = 40 - 49, 24 female),
40 50- to 59-year-olds (mean = 54.38 years, SD = 3.04, range = 50 - 59, 21 female),
40 60- to 69-year-olds (mean = 63.73 years, SD = 2.56, range = 60 - 69, 21 female), and
40 70- to 79-year-olds (mean = 72.75 years, SD = 2.44, range = 70 - 79, 20 female).

For completing the study, subjects were paid above the fixed minimum wage (on average Â£10.00 per hour; see Supplements for further detail).

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

We used the TANGO task (Prein, Bohn, Kalinke, \& Haun, 2022).
The task was presented as an interactive web application (see Figure \ref{fig:fig1}; live demo \href{https://ccp-odc.eva.mpg.de/tango-demo/.}{https://ccp-odc.eva.mpg.de/tango-demo/}; source code \url{https://github.com/ccp-eva/tango-demo}).
The TANGO showed satisfactory internal consistency and retest reliability {[}with reliability estimates \emph{Pearson's r} ranging from .5 to .8 for the categorical task version and .7 to .8 for the continuous task version; Prein et al. (2022){]}.

Each trial presented an agent standing in a window, watching a balloon (\emph{i.e.}, target) falling to the ground.
Depending on the task version, the target either fell into a box (categorical task version) or behind a hedge (continuous task version).
The agent's gaze followed the target's trajectory: pupil and iris moved so that their center aligned with the target center.
The target flight was covered so that participants could not see where the target landed.
Participants' task was to locate the target by tracking the agent's gaze.
They could respond by clicking or touching on the screen.

Four familiarization trials ensured that participants understood the task and felt comfortable with the response format.
Then, 15 test trials followed.
Completing the 19 trials took approximately 5-10 minutes.

The outcome measure depended on the task version.
In the continuous \emph{hedge version}, we assessed imprecision defined as the absolute difference between the target center and the x coordinate of the participant's click.
In the discrete \emph{box version}, we calculated the proportion of correct responses.

The randomization of the final target location also depended on the task version.
In the hedge version, the screen width was divided into ten bins, while exact coordinates within each bin were randomly generated during runtime.
In the box version, the target randomly landed in one of the boxes.
We adjusted the task difficulty according to participants' age: children were presented with five boxes while adults were presented with eight boxes.
Each bin/box, as well as all agents and target colors, occurred equally often and and did not appear in more than two consecutive trials.



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../figures/procedure} 

}

\caption{\textbf{Setup of the TANGO}. (A) Infrastructure for online testing. (i) Subjects aged 3 -- 99+ can participate. Data collection can take place anywhere: online, in kindergartens, or research labs. (ii) The task is presented as a website that works across devices. (iii) The scripts for the website and the recorded data are stored on secure local servers. (B) Hedge version (continuous) of the gaze understanding task. (i) The agent stands in a window with the target in front of them. (ii) A hedge grows and covers the target. (iii) The target falls to a random location on the ground. The agent's eyes track the movement of the target. (C) Box version (discrete) of the gaze understanding task. Number of boxes (min. 1; max. 8) as potential hiding locations can be set according to the researcher's need.}\label{fig:fig1}
\end{figure}

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

In the in-person sample, children were tested on a tablet in a quiet room in their kindergarten.
An experimenter guided the child through the study.
Half of the sample completed the continuous task version, while the other half completed the discrete task version.

In the remote sample, children and teenagers received a personalized link to the study website.
Caregivers were asked to provide technical support whenever needed, while explicitly being reminded to not help their children in responding.
Webcam videos were recorded whenever consented and technically feasible, in order to monitor whether children and teenagers responded on their own.

In the remote sample for children from three to five years old, the family's presentation device determined the task version.
Whenever a touchscreen was available, the continuous task version was presented.
For families using a computer without touchscreen, the discrete task version was shown.
Here, children were asked to point to the screen while the caregivers should execute the clicking for their children.

The number of teenagers in our internal database for recruiting families substantially decreased by age.
To avoid small sample sizes, we decided to only employ the continuous task version for our 7- to 17-year-olds.
Hence, children aged 7 years+ completed the continuous task version, independently from their presentation device.

The remote adult sample was equally distributed across the two task versions.

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

\begin{itemize}
\tightlist
\item
  TODO: non-linear model for development
\end{itemize}

All test trials without voice-over description were included in our analyses.
We ran all analyses in R version 4.3.0 (2023-04-21) (R Core Team, 2022).
Regression models were fit as Bayesian generalized linear mixed models (GLMMs) with default priors for all analyses, using the function \texttt{brm} from the package \texttt{brms} (BÃ¼rkner, 2017, 2018).

To estimate the developmental trajectory of gaze understanding and the effect of data collection mode, we fit a GLMM predicting the task performance in each trial by age (in months, z-transformed) and data collection mode (reference category: in-person supervised).
The model included random intercepts for each participant and each target position, and a random slope for symmetric target position within participants (model notation in \texttt{R:\ performance\ \textasciitilde{}\ age\ +\ datacollection\ +\ (symmetricPosition\ \textbar{}\ subjID)\ +\ (1\ \textbar{}\ targetPosition)}).
Here, \texttt{targetPosition} refers to the exact bin/box of the target, while \texttt{symmetricPosition} refers to the absolute distance from the stimulus center (i.e., smaller value meaning more central target position).
We expected that trials could differ in their difficulty depending on the target centrality and that these item effects could vary between participants.

For the hedge version, performance was defined as the absolute click distance between the target center and the click X coordinate, scaled according to target widths, and modeled by a \texttt{lognormal} distribution.
For the box version, the model predicted correct responses (0/1) using a \texttt{Bernoulli} distribution with a logit link function.
We inspected the posterior distribution (mean and 95\% Confidence Interval (CI)) for the age and data collection estimates.

\hypertarget{results}{%
\subsection{Results}\label{results}}



\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../figures/lifespan_plot} 

}

\caption{\textbf{Differentiation in gaze understanding}. Performance is measured as imprecision, i.e., the absolute distance between the target's center and the participant's click (averaged across trials). The unit of imprecision is counted in the width of the target, i.e., a participant with imprecision of 1 clicked on average one target width to the left or right of the true target center.}\label{fig:fig2}
\end{figure}

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

Three-year-olds were surprisingly inaccurate in their responses.
One possible explanation could be that they simply lacked the ability to complete the task, potentially due to issues in gaze following.
Contrasting our results with previous findings on infant gaze following, this explanation is unlikely.
A more likely explanation would be that children were able to follow the agent's gaze but struggled to translate this implicit understanding into active behavior.

Another point to keep in mind is that we used subtle eye movements as cues.
Many existing studies let the agents move eye and head in parallel, therefore establishing a confound with greater (head) movement.
Relying exclusively on the eye movement might be trickier for children than when presented with a combined eye and head orientation.

The performance of the youngest children seems more consistent with performance demands than with a failure in gaze following.

\hypertarget{computational-cognitive-model}{%
\section{Computational cognitive model}\label{computational-cognitive-model}}

\begin{itemize}
\tightlist
\item
  model auch fÃ¼r warum Ã¤ltere Erwachsene schlecht? anders schlecht als kinder?
\item
  entwicklungsperspektive basierend auf model
\item
  dann prozess-ebene = magnet
\end{itemize}

\hypertarget{participants-1}{%
\subsection{Participants}\label{participants-1}}

The sample included the 58 three- to five-year-old children and 240 adults from our Lifespan study. For age distributions, see Participant section of the Lifespan Study.

\hypertarget{non-social-vector-following}{%
\section{Non-social vector following}\label{non-social-vector-following}}

Task design, data collection, and sample sizes were pre-registered: \url{https://osf.io/xsqkt}.

The study design and procedure obtained ethical clearance by the MPG Ethics commission Munich, Germany, falling under a packaged ethics application (Appl. No.~2021\_45), and was approved by an internal ethics committee at the Max Planck Institute for Evolutionary Anthropology.
The research adheres to the legal requirements of psychological research with children in Germany.
Data were collected between February and March 2023.

\hypertarget{participants-2}{%
\subsection{Participants}\label{participants-2}}

The sample consisted of 102 children (mean age = 4.54 years, SD = 0.31, range = 3.99 - 5.03, 54 girls),

\begin{itemize}
\item
  gut, dass unser maÃ ind diff messen kann, das nutzen wir jetzt
\item
  zsmhang mit vector following, aber nicht redundant
\item
  perspective-taking auch, aber auch nicht relevant, nicht theory of mind score
\item
  diskutieren dass die maÃe auch nict ind diff maÃe sind
\item
  model vergleich: welche komponente stecken in gafo drin?,
  welchen beitrag liefern die um score zu erklÃ¤ren?
  welche wichtiger, tom oder nur perspective taking
\item
  magnet versuch das vector following zu isolieren.
  ist da noch was anderes auÃerhalb vector-following?
\item
  aggregierte messungen immer besser, nur indikatoren fÃ¼r konstrukt
\end{itemize}

\hypertarget{limitations}{%
\section{Limitations}\label{limitations}}

\hypertarget{discussion-1}{%
\section{Discussion}\label{discussion-1}}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\newpage

\hypertarget{declarations}{%
\section{Declarations}\label{declarations}}

\hypertarget{open-practices-statement}{%
\subsection{Open practices statement}\label{open-practices-statement}}

The web application (\url{https://ccp-odc.eva.mpg.de/tango-demo/}) described here is open source (\url{https://github.com/ccp-eva/tango-demo}).
The data sets generated during and/or analysed during the current study are available in the {[}gazecues-modeling{]} repository (\url{https://github.com/ccp-eva/gazecues-modeling}).
All experiments were pre-registered (\url{https://osf.io/zjhsc/}).

\hypertarget{funding}{%
\subsection{Funding}\label{funding}}

This study was funded by the Max Planck Society for the Advancement of Science, a noncommercial, publicly financed scientific organization (no grant number).
We thank all the children, caregivers, and adults who participated in the study.
We thank Jana Jurkat for her help with data collection.

\hypertarget{conflicts-of-interest}{%
\subsection{Conflicts of interest}\label{conflicts-of-interest}}

The authors declare that they have no conflict of interest.

\hypertarget{consent-to-participate}{%
\subsection{Consent to participate}\label{consent-to-participate}}

Informed consent was obtained from all individual participants included in the study or their legal guardians.

\hypertarget{authors-contributions}{%
\subsection{Authors' contributions}\label{authors-contributions}}

The authors made the following contributions: \#\#\# TODO

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-astor2022gaze}{}}%
Astor, K., \& GredebÃ¤ck, G. (2022). Gaze following in infancy: {Five} big questions that the field should answer. In \emph{Advances in {Child Development} and {Behavior}} (p. S0065240722000192). {Elsevier}. \url{https://doi.org/10.1016/bs.acdb.2022.04.003}

\leavevmode\vadjust pre{\hypertarget{ref-behne2005oneyearolds}{}}%
Behne, T., Carpenter, M., \& Tomasello, M. (2005). One-year-olds comprehend the communicative intentions behind gestures in a hiding game. \emph{Developmental Science}, \emph{8}(6), 492--499. \url{https://doi.org/10.1111/j.1467-7687.2005.00440.x}

\leavevmode\vadjust pre{\hypertarget{ref-brooks2002importance}{}}%
Brooks, R., \& Meltzoff, A. N. (2002). The importance of eyes: {How} infants interpret adult looking behavior. \emph{Developmental Psychology}, \emph{38}(6), 958--966. \url{https://doi.org/10.1037/0012-1649.38.6.958}

\leavevmode\vadjust pre{\hypertarget{ref-burkner2017brms}{}}%
BÃ¼rkner, P.-C. (2017). Brms: {An R Package} for {Bayesian Multilevel Models Using Stan}. \emph{Journal of Statistical Software}, \emph{80}(1). \url{https://doi.org/10.18637/jss.v080.i01}

\leavevmode\vadjust pre{\hypertarget{ref-burkner2018advanced}{}}%
BÃ¼rkner, P.-C. (2018). Advanced {Bayesian Multilevel Modeling} with the {R Package} brms. \emph{The R Journal}, \emph{10}(1), 395. \url{https://doi.org/10.32614/RJ-2018-017}

\leavevmode\vadjust pre{\hypertarget{ref-butterworth1991minds}{}}%
Butterworth, G., \& Jarrett, N. (1991). What minds have in common is space: {Spatial} mechanisms serving joint visual attention in infancy. \emph{British Journal of Developmental Psychology}, \emph{9}(1), 55--72. \url{https://doi.org/10.1111/j.2044-835X.1991.tb00862.x}

\leavevmode\vadjust pre{\hypertarget{ref-carpenter1998social}{}}%
Carpenter, M., Nagell, K., \& Tomasello, M. (1998). \href{https://www.ncbi.nlm.nih.gov/pubmed/9835078}{Social cognition, joint attention, and communicative competence from 9 to 15 months of age}. \emph{Monographs of the Society for Research in Child Development}, \emph{63}(4), i--vi, 1--143.

\leavevmode\vadjust pre{\hypertarget{ref-coelho2006searching}{}}%
Coelho, E., George, N., Conty, L., Hugueville, L., \& Tijus, C. (2006). Searching for asymmetries in the detection of gaze contact versus averted gaze under different head views: A behavioural study. \emph{Spatial Vision}, \emph{19}(6), 529--545. \url{https://doi.org/10.1163/156856806779194026}

\leavevmode\vadjust pre{\hypertarget{ref-colombo2001development}{}}%
Colombo, J. (2001). The development of visual attention in infancy. \emph{Annual Review of Psychology}, \emph{52}, 337--367. \url{https://doi.org/10.1146/annurev.psych.52.1.337}

\leavevmode\vadjust pre{\hypertarget{ref-dentremont1997demonstration}{}}%
D'Entremont, B., Hains, S. M. J., \& Muir, D. W. (1997). A demonstration of gaze following in 3- to 6-month-olds. \emph{Infant Behavior and Development}, \emph{20}(4), 569--572. \url{https://doi.org/10.1016/S0163-6383(97)90048-5}

\leavevmode\vadjust pre{\hypertarget{ref-frischen2007gaze}{}}%
Frischen, A., Bayliss, A. P., \& Tipper, S. P. (2007). Gaze cueing of attention: {Visual} attention, social cognition, and individual differences. \emph{Psychological Bulletin}, \emph{133}(4), 694--724. \url{https://doi.org/10.1037/0033-2909.133.4.694}

\leavevmode\vadjust pre{\hypertarget{ref-gathercole2004structure}{}}%
Gathercole, S. E., Pickering, S. J., Ambridge, B., \& Wearing, H. (2004). The {Structure} of {Working Memory From} 4 to 15 {Years} of {Age}. \emph{Developmental Psychology}, \emph{40}, 177--190. \url{https://doi.org/10.1037/0012-1649.40.2.177}

\leavevmode\vadjust pre{\hypertarget{ref-itakura1998use}{}}%
Itakura, S., \& Tanaka, M. (1998). Use of experimenter-given cues during object-choice tasks by chimpanzees ({Pan} troglodytes), an orangutan ({Pongo} pygmaeus), and human infants ({Homo} sapiens). \emph{Journal of Comparative Psychology}, \emph{112}(2), 119--126. \url{https://doi.org/10.1037/0735-7036.112.2.119}

\leavevmode\vadjust pre{\hypertarget{ref-lee1998children}{}}%
Lee, K., Eskritt, M., Symons, L. A., \& Muir, D. (1998). Children's use of triadic eye gaze information for "mind reading". \emph{Developmental Psychology}, \emph{34}(3), 525--539. \url{https://doi.org/10.1037//0012-1649.34.3.525}

\leavevmode\vadjust pre{\hypertarget{ref-moll200412}{}}%
Moll, H., \& Tomasello, M. (2004). 12- and 18-month-old infants follow gaze to spaces behind barriers. \emph{Developmental Science}, \emph{7}(1), F1--F9. \url{https://doi.org/10.1111/j.1467-7687.2004.00315.x}

\leavevmode\vadjust pre{\hypertarget{ref-moore1997role}{}}%
Moore, C., Angelopoulos, M., \& Bennett, P. (1997). The role of movement in the development of joint visual attention. \emph{Infant Behavior and Development}, \emph{20}(1), 83--92. \url{https://doi.org/10.1016/S0163-6383(97)90063-1}

\leavevmode\vadjust pre{\hypertarget{ref-palan2018prolific}{}}%
Palan, S., \& Schitter, C. (2018). Prolific.ac\textemdash{{A}} subject pool for online experiments. \emph{Journal of Behavioral and Experimental Finance}, \emph{17}, 22--27. \url{https://doi.org/10.1016/j.jbef.2017.12.004}

\leavevmode\vadjust pre{\hypertarget{ref-povinelli1997exploitation}{}}%
Povinelli, D. J., Reaux, J. E., Bierschwale, D. T., Allain, A. D., \& Simon, B. B. (1997). Exploitation of pointing as a referential gesture in young children, but not adolescent chimpanzees. \emph{Cognitive Development}, \emph{12}(4), 423--461. \url{https://doi.org/10.1016/S0885-2014(97)90017-4}

\leavevmode\vadjust pre{\hypertarget{ref-prein2022tango}{}}%
Prein, J. C., Bohn, M., Kalinke, S., \& Haun, D. B. M. (2022). \emph{{TANGO}: {A} reliable, open-source, browser-based task to assess individual differences in gaze understanding in 3 to 5-year-old children and adults}. {PsyArXiv}. \url{https://doi.org/10.31234/osf.io/vghw8}

\leavevmode\vadjust pre{\hypertarget{ref-rcoreteam2022language}{}}%
R Core Team. (2022). \emph{R: {A} language and environment for statistical computing} {[}Manual{]}. {Vienna, Austria}: {R Foundation for Statistical Computing}.

\leavevmode\vadjust pre{\hypertarget{ref-raviv2018developmental}{}}%
Raviv, L., \& Arnon, I. (2018). The developmental trajectory of children's auditory and visual statistical learning abilities: Modality-based differences in the effect of age. \emph{Developmental Science}, \emph{21}(4), e12593. \url{https://doi.org/10.1111/desc.12593}

\leavevmode\vadjust pre{\hypertarget{ref-scaife1975capacity}{}}%
Scaife, M., \& Bruner, J. S. (1975). The capacity for joint visual attention in the infant. \emph{Nature}, \emph{253}(5489), 265--266. \url{https://doi.org/10.1038/253265a0}

\leavevmode\vadjust pre{\hypertarget{ref-silverstein2021infants}{}}%
Silverstein, P., Feng, J., Westermann, G., Parise, E., \& Twomey, K. E. (2021). Infants {Learn} to {Follow Gaze} in {Stages}: {Evidence Confirming} a {Robotic Prediction}. \emph{Open Mind}, \emph{5}, 174--188. \url{https://doi.org/10.1162/opmi_a_00049}

\leavevmode\vadjust pre{\hypertarget{ref-tomasello2007reliance}{}}%
Tomasello, M., Hare, B., Lehmann, H., \& Call, J. (2007). Reliance on head versus eyes in the gaze following of great apes and human infants: The cooperative eye hypothesis. \emph{Journal of Human Evolution}, \emph{52}(3), 314--320. \url{https://doi.org/10.1016/j.jhevol.2006.10.001}

\leavevmode\vadjust pre{\hypertarget{ref-zhang2019role}{}}%
Zhang, X., Zhang, Z., Zhang, Z., Tang, Y., \& Liu, W. (2019). The role of the motion cue in the dynamic gaze-cueing effect: {A} study of the lateralized {ERPs}. \emph{Neuropsychologia}, \emph{124}, 151--160. \url{https://doi.org/10.1016/j.neuropsychologia.2018.12.016}

\end{CSLReferences}

\endgroup

\newpage


\end{document}
